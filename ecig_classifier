#! /usr/bin/env python
# -*- coding: utf-8 -*-
import sys
import re
import nltk
import os
from nltk.tokenize import RegexpTokenizer
from nltk import bigrams, trigrams
import math
import numpy as np
import copy 
from warnings import catch_warnings
from scipy.sparse import *
from scipy import *
from sklearn import svm, cross_validation
from sklearn.cross_validation import StratifiedKFold
from sklearn import linear_model
from sklearn.linear_model.base import LinearModel
from sklearn.linear_model.coordinate_descent import LinearModelCV
from sklearn import metrics
from sklearn.metrics import classification_report
import pickle
import random
from scipy.sparse import hstack
from scipy.sparse import vstack
from sklearn.feature_extraction.text import CountVectorizer
import operator
from sklearn import svm
import pickle
from sklearn import preprocessing
from os.path import relpath
import tldextract
import subprocess
import itertools
import scipy as sp
import scipy.sparse as sps
from multiprocessing import Pool,Lock,Manager
from score_measurement import scoreMeasurement
from feature_generator import generates_features
from feature_generator import storeModels
from pos_tag_operations import get_pos_tags
from pos_tag_operations import get_consecutive_pos_tag
from polarity_detector import get_polarity_features 
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import Normalizer
from sklearn import svm
from combination_creator import get_efficient_combinations
from combination_creator import validation_check
from process_pool import work_with_combinations
from process_pool import grab_result
from polarity_detector import load_sent_word_net
from scipy.sparse import csr_matrix
import pickle
from sklearn.feature_extraction import DictVectorizer
from collections import Counter
from feature_generator import generates_features_transform
from process_pool import parse_resultant_output
from process_pool import train_model
from process_pool import test_model
from process_pool import train_test_operations
from process_pool import with_shuffle_operation
from process_pool import work_with_folds
from nltk import word_tokenize
from nltk import sent_tokenize
from nltk.stem import SnowballStemmer
stopwords = nltk.corpus.stopwords.words('english')
matrix = None #csr_matrix()
class_label = np.array([])
dictUrls = {}
english_stemmer = SnowballStemmer('english')
indexingDic = {}
normalizer = 1
maximus = -1
labelList = []
flavors = {'cherry':0, 'cinnamon':0,'mint':0,'grape':0,'blueberry':0,'clove':0,'citrus':0,'bubblegum':0,'strawberry':0,'banana':0,'goldrush':0,'menthol':0,'lemonade':0,'watermelon':0,'cola':0,'caramel':0,'toffee':0,
'fudge':0,
'brownie':0,
'cheesecake':0,
'coffee':0,
'chocolate':0,
'green tea':0}
quit_smoking = {'quit':0,'smoking cessation':0,'stop smoking':0}
reduced_harm = {'harm[- ]reduction':0, 'reduced[- ]harm':0, 'less[- ]harmful':0,'low[- ]risk':0}
flavors_count = { }
class result_collector():
    def __init__(self,P_ = 0,R_ = 0,F1_ = 0):
	self.P = P_
	self.R = R_
	self.F1 = F1_
	self.beta = 0.0
	self.best_combination = None
	return
class fScore(object):
    p = 0
    r = 0
    f1 = {}
    def __init__(self,Tp_ = 0,Fp_ = 0, Tn_ = 0, Fn_ = 0,expecresultList_ = None,precresultList_ = None):
        self.Tp = Tp_
        self.Fp = Fp_
        self.Tn = Tn_
        self.Fn = Fn_
        self.expecresultList = expecresultList_
        self.preresultList = precresultList_
        return


class objectDataContainer(object):
    def __init__(self,X= None,y=None,y_rank = None,latest_tweets = None,user_names_ = None):
        self.X = X
        self.y = y
        self.y_rank = y_rank
        self.latest_tweets = latest_tweets
        self.hash_tag = None
	self.len_twits = None
	self.user_names = user_names_
        return
    
class urlsComponent(object):
	def __init__(self,domain = None,subDomain = None,suffix = None,list_data = None):
		self.domain = domain
		self.subdomain = subdomain
		self.suffix = suffix
		self.list_data = list_data
		self.dict_pointer = {'dom':self.domain,'subdom':self.subdomain,'suf':self.suffix,
		'lis_data':list_data}
		return
		return
	def set_ops(self,op_mode = 'lis_data',start = 0):
		self.get_val = op_mode
		self.start = start
		self.end = len(self.dict_pointer[self.get_val])
		return
	def __iter__(self):
		return self
	def next(self):
		if(self.start >= self.end):
			return None
		current = self.dict_pointer[self.get_val][self.start]
		self.start += 1
		return current
def startReordering(fileName):
    data = []
    file_object = open(fileName)
    try:
        data = file_object.readlines()
    finally:
        file_object.close()
    file_object_write = open("C:\\TwitterBigData\\tweetMessage\\tweetContentQuitkeyWords3Update.txt","w")
    try:
        for dat in data:
            tempData = dat.split("\t")
            if(len(tempData) == 3):
                file_object_write.write(tempData[1]+"\t"+tempData[0]+"\t"+tempData[2])
            elif(len(tempData) == 2):
                file_object_write.write(tempData[1]+"\t"+tempData[0])
            else:
                file_object_write.write(tempData[0])              
    finally:
        file_object_write.close()
    return
def startOperatingandLoad():
    urls_vocabulary = list()
    global dictUrls
    for i in range(0,6):
        file_object_urls = open('data/urlsuExpandedOutput'+str(i)+'.txt')
        try:
            data = file_object_urls.readlines()
        finally:
            file_object_urls.close()
        urls_vocabulary.extend(data)
        #print(len(urls_vocabulary))
    
    for dat in urls_vocabulary:
        stringVal = dat.split("\t")     
        if(len(stringVal) == 5 or len(stringVal) == 4):
            stringVal[2].strip()
            dictUrls[stringVal[2]] =stringVal[3].strip()
            #print(dictUrls[stringVal[2]])
        else:
            dictUrls[stringVal[2]] = None;
    return
def domain_extractor(url_name):
	urls = tldextract.extract(url_name)
	tempList = re.findall('[a-zA-Z0-9]+',url_name)
	tempList = [tokens for tokens in tempList if len(tokens) > 2]
	if(tempList.count('http') != 0 ):
		tempList.remove('http')
	if(tempList.count(urls.domain) != 0):
		tempList.remove(urls.domain)
	if(tempList.count(urls.subdomain) != 0):
		tempList.remove(urls.subdomain)
	if(tempList.count(urls.suffix) != 0):
		tempList.remove(urls.suffix)
	url_object = urlsComponent(domain_= urls.domain,subDomain_ = urls.subdomain,suffix_ = urls.suffix,list_data = tempList)
	return url_object

def startParsingField(data):
    global indexingDic
    global dictUrls
    global normalizer
    labelList
    strings = 'doc'
    #previously used the following line
    #file_object_read = open("C:\\TwitterBigData\\tweetMessage\\combinedLabelTrainedClassesW2.txt")
    '''file_object_read = open(dir)
    try:
        data = file_object_read.readlines()
    finally:
        file_object_read.close()'''
    i = 0
    j = 0
    dictionaryForDoc = {}
    # file_object_Towrite = open("C:\\TwitterBigData\\tweetMessage\\positiveUrls.txt","w");
    countIndicator = 0
   
    for dat in range(0,len(data)):
        # tempData = data[dat].split("\t")
        tempSubData = data[dat].split(' ')#[dat][0].split(" ")
        
        #print(tempSubData)
        # the following line for labeling we dont need until we do supervised classification
        # labelList.append(tempData[3])
        #del tempData
        tempString = ""
        dictForFeatures = {}
        indicator = 0
        for tempSubDat in tempSubData:
	    if(len(tempSubDat) == 0 ): continue
            tempSubDat = tempSubDat.replace("\n","")
            if(tempSubDat[0] != "@"):
              #  print('hashTag#: ,' + tempSubDat)
                tempSubDat =  tempSubDat.replace("@","")
                if(dictForFeatures.get(1,0) == 0):
                    dictForFeatures[1]= set()
                dictForFeatures[1].add(tempSubDat)
		tempString = tempString + ' ' + tempSubDat
            elif(tempSubDat[0] != "#"):
                # print('userMentions: ' + tempSubDat)
                tempSubDat = tempSubDat.replace("#","")
                if(dictForFeatures.get(2,0) == 0):
                    dictForFeatures[2] = set()
                if(indicator == 0):
                    indicator += 1
                    countIndicator += 1
                dictForFeatures[2].add(tempSubDat)
		tempString = tempString +' ' + tempSubDat
            elif(re.search("(?P<url>https?://[^\s]+)", tempSubDat) != None):
                # print('urls are: ' + tempSubDat)
                if(dictForFeatures.get(3,0) == 0):
                    dictForFeatures[3] = list()
                tempSubDat = tempSubDat.strip()
		#dictForFeatures[3].add(dictUrls.get(tempSubDat,tempSubDat))
		url_object = domain_extractor(dictUlrs.get(tempSubDat,tempSubData))
		dictForFeatures[3].append(url_object.domain)#dictUrls.get(tempSubDat,tempSubDat))
		tempString = tempString + ' ' + url_object.domain 
		dictForFeatures[3].append(url_object.subdomain)
		tempString = tempString +' '+ url_object.subdomain
		dictForFeatures[3].append(url_object.suffix)
		tempString = tempString +' '+ url_object.suffix
		for components in url_object.list_data:
			dictForFeatures[3].append(components.strip())
			tempString = tempString + ' ' + components
		#tempString = tempString + dictUrls.get(temp)
                # if(dictUrls.get(tempSubDat,None) == None):
                    #file_object_Towrite.write(tempData[0]+"\t"+tempData[1]+"\t"+tempSubDat+"\n")
            else:
                if(dictForFeatures.get(4,0) == 0):
                    dictForFeatures[4] = list();
                #print('Simple text: ' + tempSubDat)
                if(tempSubDat.lower().find('e-cig')!= -1 or tempSubDat.lower().find('e-j')!= -1 or tempSubDat.lower().find('e-*')):
                    tempSubDat = tempSubDat.replace('-','')
		#if(len(tempSubDat) > 2 and tempSubDat not in stopwords):
		#dictForFeatures[4].append(tempSubDat)
                    #print(tempSubDat)
                tempString = tempString + tempSubDat+" "
		#dictForFeatures[4].append(tempString.split(" "))
            del tempSubDat
        del tempSubData
	#tempString = tempString.split(" ")
	dictForFeatures[4] = tempString.split(' ')
	'''if(dictForFeatures.get(4,0) == 0):
		dictForFeatures[4] = list()
	for each in tempString:
		dictForFeatures[4].append(each)'''
        try:        
            tokens = [token.lower() for token in tempString if len(token) > 2]
        except:
            print(sys.exc_info())
        #tokens = [token for token in tokens if token not in stopwords]
        ''' bi_tokens = bigrams(tokens);
        tri_tokens = trigrams(tokens)
        bi_tokens =[' '.join(token) for token in bi_tokens]
        tri_tokens = [' '.join(token) for token in tri_tokens]
        
        for elements in bi_tokens:
            dictForFeatures[4].add(elements)
        for elements in tri_tokens:
            dictForFeatures[4].add(elements)'''
        dictionaryForDoc[strings+str(i)] = dictForFeatures #,data[dat][1])
        i = i + 1
        #print('iteration number: '+str(i)+"\n")
   	# for dat in range(0,len( dictionaryForDoc)):
        #print(dictionaryForDoc[strings+str(dat)])
    del data
    
    j = 1
    overAll = 0
    total = 0
    print('countIndicator: '+ str(countIndicator))
    global maximus
   # print("maxx indexing: "+str(j)+'total items: '+str(overAll)+"maximum feature: "+ str(maximus))       
    return dictionaryForDoc
# following function combining the classes for preparing the data set for train.


    # creating the matrix for feeding into the classifier
def class_labels():
    global labelList
    print('start class lebels')
    class_label = np.array(labelList)
    print('shape: '+ str(class_label.shape))
    print('dimension: '+ str(class_label.ndim))
    for data in range(0,100):
        print('class lebel '+str(data)+': '+ class_label[data])
    return
def dataFormatandMatrix():
    global dictionaryForDoc
    global indexingDic
    global normalizer
    global maximus
    global labelList
    global matrix
    global class_label
    # rowList = list()
    # colsList = list()
    # dataList =list()
    
    #print("Normalizer: " + str(normalizer))
    print("Indexing Dictionary: "+ str(len(indexingDic)))
    matrix = csr_matrix((len(dictionaryForDoc),len(indexingDic)+1))
    write_matrix = open("data/model/matrix","wb")
    #class_label = csr_matrix(len(dictionaryForDoc),1).todense()
    class_label = np.array(labelList)
    matrix = lil_matrix(matrix)
    for i in range(0,len(dictionaryForDoc)):
        tempDict = dictionaryForDoc['doc' + str(i)]
        for items in tempDict.values():
            for item in items:
                matrix[i,indexingDic[item]] = 1                
                #dataList.append(indexingDic[items]/normalizer)
    matrix = matrix.tocsr()
    pickle.dump(matrix,write_matrix, pickle.HIGHEST_PROTOCOL)
    #matrix = matrix.todense()
    #rows = np.array(rowList)
    #cols = np.array(colsList)
    #data = np.array(dataList)
    #print(str(len(rows))+" "+str(len(cols))+" "+ str(len(data)) )
    print(str(matrix.shape)+" "+str(matrix.ndim))
    return
def storeModels(clf,i,c):
    model_saver = open("data/model/model_"+c+str(i),"wb")
    pickle.dump(clf, model_saver, pickle.HIGHEST_PROTOCOL)
    return
def loadMatrix():
    mat = pickle.load("data/model/matrix")
    #print(mat)
def loadModels(fileName):
    file_to_read = open(fileName,'rb')
#try:
    clf = pickle.load(file_to_read)
#   except:
#    	clf = None# linear_model.LogisticRegression()
    return clf
def svmClassifier(Xtrain,X_test):
    X =[]
    y =[]
    y_rank = []
    Xtest = []
    Ytest = []
    Yranktest = []
    for elem in range(0,len(Xtrain)):
        X.append(Xtrain[elem][0])
        y.append(Xtrain[elem][1][0])
        y_rank.append(Xtrain[elem][1][1])
    for i in xrange(0,len(X)):
        print("print X ,y "+ str(X[i])+' ' + str(y[i]))
    clf = svm.SVC(kernel='rbf')
    vectorizer = CountVectorizer(min_df = 1,stop_words='english',ngram_range = (1,2))
    matrix = vectorizer.fit_transform(X)
    #  for elem in xrange(0,len(y_rank)):
    #      y_rank[elem] = math.log(float(y_rank[elem]))
    #  matrix = hstack([matrix,y_rank])
    write_score = open("data/profileClassificationreport5.txt","w+")
    matrix = matrix.tocsr()
    y = np.array(y)
    score = clf.fit(matrix,y)
    #print('score:' + score)
    for elem in xrange(0,len(X_test)):
        Xtest.append(X_test[elem][0])
        Ytest.append(X_test[elem][1][0])
        Yranktest.append(X_test[elem][1][1])
    test_mat = vectorizer.transform(Xtest)
    #  for elem in xrange(0,len(Yranktest)):
    #     Yranktest[elem] = math.log(float(Yranktest[elem]))
    # test_mat = hstack([test_mat,Yranktest])
    predict = clf.predict(test_mat)
    write_score.write(classification_report(np.array(Ytest),predict))
    write_score.close()
    return
'''def plotCurve(X,y,model):
    plt.figure()
    plt.title("Learning Curve (Naive Bayes)")
    plt.xlabel("Training examples")
    plt.ylabel("Score")
    train_sizes, train_scores, test_scores = learning_curve(model, X, y, cv=10, n_jobs=1)
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)
    plt.plot(train_sizes, train_scores_mean, label="Training score", color="r")
    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                 train_scores_mean + train_scores_std, alpha=0.2, color="r")
    plt.plot(train_sizes, test_scores_mean, label="Cross-validation score",   color="g")
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,test_scores_mean + test_scores_std, alpha=0.2, color="g")
    plt.legend(loc="best")
    plt.figure()
    plt.title("Learning Curve (SVM, RBF kernel, $\gamma=0.001$)")
    plt.xlabel("Training examples")
    plt.ylabel("Score")
    return'''
def startTesting(X_test,param):
    print('Start Testing\n')
    Xtest = []
    Ytest = []
    Yranktest = []
    latest_tweets = []
    latest_tweets_test = []
    hash_tag = []
    hash_tag_test = []
    len_tweets_test = []
    picked_alpha = 0.85
    file_model_reader = open('data/model/reg_model'+str(param),'rb')
    file_model_reader_latest = open('data/model/reg_model_latest' +str(param),'rb')
    file_vectorizer_reader = open('data/model/vectorizer'+str(param),'rb')
    file_vectorizer_latest_reader = open('data/model/vectorizer_latest'+str(param),'rb')
    try:
        logRegression = pickle.load(file_model_reader)
	logRegression_latest = pickle.load(file_model_reader_latest)
        vectorizer = pickle.load(file_vectorizer_reader)
        vectorizer_for_latest_tweets = pickle.load(file_vectorizer_latest_reader)
    except pickle.PickleError:
        print(pickle.PickleError.message())
    except ValueError:
        print('Could not handle data')
    except :
        print('unexpected error: ' +str( sys.exc_info()[0]))
        
    write_score = open("data/profileClassificationreport_hash_tag.txt","w+")
    for elem in xrange(0,len(X_test.X)):
        Xtest.append(X_test.X[elem])
        Ytest.append(X_test.y[elem])
        Yranktest.append(X_test.y_rank[elem])
        hash_tag_test.append([X_test.hash_tag[elem]])
	X_test.latest_tweets[elem] = X_test.latest_tweets[elem].encode('utf-8','ignore')
        latest_tweets_test.append(X_test.latest_tweets[elem].decode('utf-8','ignore'))
	len_tweets_test.append([X_test.len_twits[elem]])
    for elem in xrange(0,len(Yranktest)):
        Yranktest[elem][0] = math.log(float(Yranktest[elem][0]+1))
    test_mat = vectorizer.transform(Xtest)
    test_mat_latest = vectorizer_for_latest_tweets.transform(latest_tweets_test)
    numarrTest = np.array(hash_tag_test)
    y_rank_test_terr = np.array(Yranktest)
    test_mat = hstack([test_mat,np.array(len_tweets_test)])
    # print(str(test_mat.get_shape()[0])+' '+ str(test_mat.get_shape()[1])+' '+ str(len(Yranktest)))
    # test_mat = hstack([test_mat,numarrTest])
    # test_mat = hstack([test_mat,y_rank_test_terr])
    #test_mat = hstack([test_mat,test_mat_latest])
    #test_mat = preprocessing.StandardScaler(with_mean = False).fit_transform(test_mat)
    predict_ = logRegression.predict_proba(test_mat)
    predict_2 = logRegression_latest.predict_proba(test_mat_latest) 
    print(str(np.shape(predict_)) + str(np.shape(predict_2)))
    final = picked_alpha*predict_ + (1- picked_alpha)* predict_2
    #print(str(final))
    final = final[:,1] > 0.5
    #predict_ = logRegression.decision_function(test_mat)
    serial = 0
    '''for elem in predict_:
        if(elem != Ytest[serial]):
            pass#print('elem at '+str(serial) +': ' + str(elem)+ 'True value: ' + str(Ytest[serial]))
        serial += 1'''
    num = np.sum(predict_ > 0.5)
    list_of_users = predict_.flatten().tolist()
    #print('pos class: '+ str(num) +'neg class: '+ str(predict_.shape[0]- num))
    #predict_result =  logRegression.predict_proba(test_mat)	
    final = 1*final
    #print(str(final))
    write_score.write(classification_report(np.array(Ytest), final))
    #metrics.classification_report(Ytest, predict_, target_names = [1,0])
    storeModels(logRegression, 1)
    write_score.close()
    #write_score.close()
    print('End of Testing')
    return list_of_users
'''# right now we have this code in seperate file
def get_ensemble_results(log_regression_p,log_regression_l,test_mat,test_mat_profile,y_test):
	picked_alpha = 0.85
	predict_p = log_regression_p.predict_proba(test_mat)
	predict_l = log_regression_l.predict_proba(test_mat_profile)
	final_result = picked_alpha*predict_p + (1 - picked_alpha)*predict_l
	final_result = final_result[:,1] > 0.5
	final_result = 1*final_result
	return final_result
def build_classifier(matrix_profile,matrix_latest_tweets,test_label,model_index,kf):
	class_label = np.array(test_label)
	list_of_results = []
	matrix_profile = matrix_profile.tocsr()
	matrix_latest_tweets = matrix_latest_tweets.tocsr()
	logRegression_p = linear_model.LogisticRegression()
	logRegression_l = linear_model.LogisticRegression()
	multinomial_p = MultinomialNB()
	gaussian_p = GaussianNB()
	svm_p = svm.SVC(kernel = 'linear')
	#multinomial_nb = lienar_model
	print('i am inside of the k-fold validation\n')
	for train_index,test_index in kf:
		X_train_p,X_test_p = matrix_profile[train_index],matrix_profile[test_index]
		X_train_l,X_test_l = matrix_latest_tweets[train_index],matrix_latest_tweets[test_index]
		Y_train,Y_test = class_label[train_index],class_label[test_index]
		logRegeression_p.fit(X_train_p,Y_train)
		logRegression_l.fit(X_train_l,Y_train)
		multinomial_p.fit(X_train_p,Y_train)
		result_final = get_ensemble_results(multinomial_p,logRegression_l)
		list_of_result.append(fScore(expecresultList_ = Y_test,precresultList_ = final_result)))
	return list_of_result
'''
def startTrainingOperation(Xtrain,param):
    X =[]
    y =[]
    y_rank = []
    Xtest = []
    Ytest = []
    Yranktest = []
    latest_tweets = []
    latest_tweets_test = []
    hash_tag = []
    hash_tag_test = []
    len_tweets = []
    for elem in range(0,len(Xtrain.X)):
        X.append(Xtrain.X[elem])
        y.append(Xtrain.y[elem])
        y_rank.append(Xtrain.y_rank[elem])
        hash_tag.append([Xtrain.hash_tag[elem]])
	Xtrain.latest_tweets[elem] = Xtrain.latest_tweets[elem].encode('utf-8','ignore')
        latest_tweets.append(Xtrain.latest_tweets[elem].decode('utf-8','ignore'))
	len_tweets.append([Xtrain.len_twits[elem]])
    #  print(Xtrain[elem][1][1])
    vectorizer = CountVectorizer(min_df = 3,stop_words='english',ngram_range = (1,2))
    vectorizer_for_latest_tweets = CountVectorizer(min_df = 3,stop_words='english',ngram_range = (1,2))
    mat_latest = vectorizer_for_latest_tweets.fit_transform(latest_tweets)
    matrix = vectorizer.fit_transform(X)
    matrix = hstack([matrix,np.array(len_tweets)])
    #matrix = hstack([matrix,mat_latest])
    for elem in xrange(0,len(y_rank)):
        y_rank[elem][0] = math.log(float(y_rank[elem][0]+1))
    #print('shape of matrix: ' + str(matrix.shape) + " shape of hash_tag: "+ str(len(hash_tag)))
    # print Xtrain
    # tmpYs = [x[0][1][0] for x in Xtrain]
    #print(str(tmpYs))
    numarrTrain = np.array(hash_tag)
    y_rank_terr = np.array(y_rank)
    # matrix = hstack([matrix,numarrTrain])
    #matrix = hstack([matrix,mat_latest])
    #matrix = preprocessing.StandardScaler(with_mean=False).fit_transform(matrix)
    #matrix = hstack([matrix,y_rank_terr])
    model_writer = open('data/model/model'+str(param),'wb')
    vectorizer_writer =  open('data/model/vectorizer'+str(param),'wb')
    vectorizer_latest_tweets = open('data/model/vectorizer_latest'+str(param),'wb')
    log_regression_model_writer = open('data/model/reg_model' + str(param),'wb')
    log_regression_model_writer_latest = open('data/model/reg_model_latest' + str(param),'wb')
    kf = cross_validation.KFold(len(y),4,indices = True)
    matrix = matrix.tocsr()
    y = np.array(y)
    svmRegression = svm.SVC(C = 0.6,kernel='poly',degree = 5,class_weight = {1:0.2,0:0.8})#,class_weight={1:0.6, 0:0.4})#linear_model.LogisticRegression(C = 0.8)
    logRegression_profile_feature = linear_model.LogisticRegression(C = 0.8)
    logRegression_tweet_feature = linear_model.LogisticRegression(C = 0.8)
    print('start operation')
    clf_object =svmRegression.fit(matrix, y)
    clf_regression_object = logRegression_profile_feature.fit(matrix, y)
    clf_regression_object_latest = logRegression_tweet_feature.fit(mat_latest,y)
    pickle.dump(clf_object, model_writer, pickle.HIGHEST_PROTOCOL)
    pickle.dump(vectorizer,vectorizer_writer,pickle.HIGHEST_PROTOCOL)
    pickle.dump(clf_regression_object,log_regression_model_writer,pickle.HIGHEST_PROTOCOL)
    pickle.dump(clf_regression_object_latest,log_regression_model_writer_latest,pickle.HIGHEST_PROTOCOL)
    pickle.dump(vectorizer_for_latest_tweets,vectorizer_latest_tweets,pickle.HIGHEST_PROTOCOL)

    '''for train_index,test_index in kf:
        X_train,X_test = matrix[train_index],matrix[test_index]
        Y_train,Y_test = y[train_index],y[test_index]
        logRegression = linear_model.LogisticRegression()
        print('start operation')
        score =logRegression.fit(X_train, Y_train)
        predictions = logRegression.predict(X_test)'''
        #expected = Y_test
    #write_score.write(classification_report(expected, predictions))
    #print("Accuracy %0.2f (+/- %0.2f)"%(score.mean(),score.std()*2))
    return vectorizer
def dict_parser(data_source):
     for range_elem in xrange(0,len(data_source)):
	     data_source[range_elem] = data_source[range_elem].replace('"','')
	     data_source[range_elem] = data_source[range_elem].replace('.',' ')
		#data_source[range_elem] = data_source[range_elem].replace('\'','')
     dict_vec = DictVectorizer()
     mat = dict_vec.fit_transform(Counter(s.split()) for s in data_source)
     arr_dict = dict_vec.inverse_transform(mat)# check for redundancy operation here
     return arr_dict

def get_dic_to_list(dictOfFeatures, dataObject):
    X_update = []
    y_update = []
    stringDoc = 'doc'
    i = 0
    hash_tag_list = []
    len_tweets = []
    for values in xrange(0,len(dictOfFeatures)):
        prepare_doc = ''
        for keyVal in dictOfFeatures[stringDoc+str(values)].keys():
            setOb = dictOfFeatures[stringDoc+str(values)].get(keyVal,None)
            if(setOb == None and keyVal == 2):
                hash_tag_list.append(0)
            elif(keyVal == 2):
                hash_tag_list.append(len(setOb))
            else:
                pass
            if(setOb != None and keyVal == 4):#and keyVal != 1 and keyVal != 2):
                #strings = ''
                for elem in setOb:
                    #strings =strings + elem +' '
                    prepare_doc += elem.strip()+' '
		len_tweets.append(len(prepare_doc))
        if(len(hash_tag_list) <= values):
            hash_tag_list.append(0)
	#prepare_doc = prepare_doc.encode('utf-8','ignore')
        X_update.append(prepare_doc.decode('utf-8','ignore'))
        #y_update.append(dictOfFeatures[stringDoc+str(values)][1])
    #print(len(X_update))
    dataObject.hash_tag = hash_tag_list
    dataObject.X = X_update
    dataObject.len_twits = len_tweets
    #print('hash_tag list: ' + str(len(hash_tag_list)))
    return dataObject#zip(zip(X_update,y_update),hash_tag_list)
def randomized_data(X_,y_,y_rank,latest_tweet,user_names):
    arr = [0]*len(X_)
    temp_X = []
    temp_y = []
    temp_y_rank = []
    temp_latest_tweets = []
    count_to_reach = 0
    while(True):
        rand = random.randint(0,len(X_)-1)
        if(arr[rand] == 0):
            arr[rand] = 1
            count_to_reach += 1
            temp_X.append(X_[rand])
            temp_y.append(y_[rand])
            temp_y_rank.append(y_rank[rand])
            temp_latest_tweets.append(latest_tweet[rand])
        if(count_to_reach == len(X_)):
            break
    temp_object = objectDataContainer(X = X_,y = y_, y_rank = temp_y_rank, latest_tweets = latest_tweet, user_names_ = user_names)#(X = temp_X,y =temp_y, y_rank = temp_y_rank, latest_tweets=temp_latest_tweets)
    return temp_object
#//////////////////////////////////////////// feature sets /////////////////////////////////////

def get_parse_topic_features(path_name,size):
	dict_for_topic_features = {}
	file_for_topics = open(path_name)
	try:
		data = file_for_topics.readlines()
	finally:
		file_for_topics.close()
	print(str(len(data)))
	matrix = np.zeros(shape=(len(data),size),dtype=float,order='F')
	for datum in data:
		data_list_items = datum.strip().split('\t')
		head,tail = os.path.split(data_list_items[1])
	        file_index = re.findall('[\d]+',tail)
		for elem in xrange(2,len(data_list_items),2):
			#print('element is :' + str(elem))
			#print('data we have now :' + data_list_items[elem+1])
			#print(str(file_index[0]) +' '+ data_list_items[elem])
			matrix[int(file_index[0])][int(data_list_items[elem])] = float(data_list_items[elem + 1])
			#print(matrix[int(file_index[0])])
		

	return matrix

def get_emoticon_count(path_name='data/source_data.txt',emoticon_file = 'data/EmoticonLookupTable.txt',\
			                       emoticon_granual = 'data/emoticonsWithPolarity.txt'):
	global polarity
	pat1 = '\(\^'
	pat2 = ' \^\)'
	file_to_read = open(path_name)
	vectorizer = CountVectorizer(min_df = 1,ngram_range=(1,2))
	emoticon_vec = {}
	emoticon_granual_vec = {}
	try:
		data = file_to_read.readlines()
	finally:
	        file_to_read.close()
	arr_dict = dict_parser(data)
	file_to_read_emoticons = open(emoticon_file)
	try:
	        emoticon_data = file_to_read_emoticons.readlines()
	finally:
	        file_to_read_emoticons.close()
	
	file_to_read_emoticon_granual = open(emoticon_granual)
	try:
	        emoticon_granual_data = file_to_read_emoticon_granual.readlines()
	finally:
	        file_to_read_emoticon_granual.close()
	for datum in emoticon_granual_data:
	        sub_data = datum.split('\t')
		type_data = sub_data[1]
		ob = re.search(pat1 + '(?=' + pat2+')', sub_data[0], re.I)
		if(ob != None):
			#print('pattern is: ' + pat1 + pat2)
			sub_sub_data = sub_data[0].split('(^ ^)')
			emoticon_granual_vec['(^ ^)'] = sub_data[1]
			sub_data[0] = sub_sub_data[0] +' '+sub_sub_data[1]
		sub_sub_data = sub_data[0].split(' ')
		for each in sub_sub_data:
			if(emoticon_granual_vec.get(each,None) == None):
				emoticon_granual_vec[each] = sub_data[1] 
																			        
	X = []
	index_ = 2 if path_name.find('latest') == -1 else 0
	for datum in data:
		sub_data = datum.split('\t')
		X.append(sub_data[index_].decode('utf-8','ignore'))
	# matrix = vectorizer.fit_transform(X)
	#reverse_arr = vectorizer.inverse_transform(matrix)
	for datum in emoticon_data:
		sub_datum = datum.split('\t')
		if(emoticon_vec.get(sub_datum[0],None)== None):
			emoticon_vec[sub_datum[0]] = sub_datum[1]
	user = {}
	#print('lenght of X is: ' + str(len(X)))																			
	#print('lenght of arr_dict is: ' + str(len(arr_dict)))
        h_arr = np.zeros((len(arr_dict),3))
	pos,neg,neu = 0,0,0
	counter_map = dict(emoticon_vec.items() + emoticon_granual_vec.items())
	for i in xrange(0,len(arr_dict)):
		for keys in arr_dict[i]:
			if(counter_map.get(keys,None) != None):#emoticon_vec.get(keys,None) != None or emoticon_granual_vec.get(keys,None) != None):
				#print('reverse doc pos and element:'+ str(i) +' '+ keys)
				if(counter_map[keys].lower().find('positive') != -1): #or counter_map[keys].lower().find('negative') != -1 or counter_map[keys].lower().find('neutral') != -1):
					h_arr[i][0] += 1
					#print('indexes :' + str(i) +' '+ str(0))
					pos += 1
				elif(counter_map[keys].lower().find('negative') != -1):
					#print('indexes :' + str(i) +' '+ str(1))
					h_arr[i][1] += 1
					neg += 1
				else:	
					# print('indexes :' + str(i) +' '+ str(2))
					h_arr[i][2] += 1					
					neu += 1
	print("number of elements have the emoticons: " + str(pos) +' ' + str(neg) +' '+str(neu))
       	return h_arr
def get_punctuation_feature(data_source):
	punctuation_feature = zeros((len(data_source),1))
	for each in xrange(0,len(data_source)):
		temp_data = data_source[each].split(' ')
		for sub_each in temp_data:
			if(len(sub_each) == 0): continue
			if(sub_each[len(sub_each)-1] == '!' or sub_each[len(sub_each)-1] == '?'):
				punctuation_feature[each] = 1
				break
	return punctuation_feature
def get_username_feature(data_source):
	key_word_list = ['vaping','vape','vapor','vapes','ecig','e-cig','ejuice','eliquid','smoke','tobacco']
	arr_username = np.zeros((len(data_source),1))
	for each in xrange(0,len(data_source)):
		temp_data = data_source[each].split('\t')
		for datum in key_word_list:
			if(temp_data[1].lower().find(datum) != -1):
				arr_username[each] = 1
				break
	return arr_username
def get_rt_features(data_source):
	print(type(data_source))
	rt_feature = zeros((len(data_source),1))
	for each in xrange(0,len(data_source)):
		datum = data_source[each].split(' ')
		for sub_each in datum:
			if(sub_each.lower() == 'rt'):
				rt_feature[each] = 1
				break
	return rt_feature
def get_hash_feature(data_source):
	hash_feature = zeros((len(data_source),1))
	for each in xrange(0,len(data_source)):
		datum = data_source[each].split(' ')
		for sub_each in datum:
			if(len(sub_each)== 0): continue
			hash_feature[each] = 1 if sub_each[0] == '#' else 0
	return hash_feature
def get_usermention_feature(data_source):
	usermention_feature = zeros((len(data_source),1))
	for each in xrange(0,len(data_source)):
		datum = data_source[each].split(' ')
		for sub_each in datum:
			if(len(sub_each) == 0): continue
			usermention_feature[each] = 1 if sub_each[0] == '@' else 0
	return usermention_feature

def read_data_from_source(file_path):
	file_object_To_read = open(file_path,'r+')
	try:
		data = file_object_To_read.readlines()
	finally:
		file_object_To_read.close()
	return data
def get_latest_tweets(latest_tweets,number_of_tweets):
	latest_tweets = {}
	if(number_of_tweets > 200):
		print('too many tweets overflowed \n')
		return
	for datum in latest_tweets:
		tempStr = datum.split('\t')
		for i in xrange(1,min((len(tempStr) - 1),number_of_tweets)): #previously it was max((len(tempStr) - 1),2)
			if(latest_tweets.get(tempStr[0],None) != None):
				data_list = tempStr.split('TimeeeeerDate:')
				latest_tweets[tempStr[0]] = latest_tweets[tempStr[0]] + data_list[0] + ' '
			else:
			 	if(len(tempStr) > 1):
					latest_tweets[tempStr[0]] = tempStr[i].split('TimeeeeerDate:')[0]
	return latest_tweets
def get_tweet_contents(tweet_contents,number_of_tweets):
	tweet_contents_dict = {}
	tweet_counter = {}
	for datum in tweet_contents:
		tempStr = datum.split('\t')
		if(tweet_counter.get(tempStr[1],None) == None):
			tweet_counter[tempStr[1]] = 0
		tweet_counter[tempStr[1]] += 1
		if(tweet_counter[tempStr[1]] < number_of_tweets):
			if(tweet_contents_dict.get(tempStr[1],None) == None):
				tweet_contents_dict[tempStr[1]] =  tempStr[3] + ' '
			else:
			 	tweet_contents_dict[tempStr[1]] = tweet_contents_dict[tempStr[1]] + tempStr[3] +' '

	return tweet_contents_dict
def get_rank_list(rank_data,train_data,cross_data,test_data):
	user_dict = {}
	rank_list = []
	for datum in rank_data:
		temp_str = datum.split(' ')
		user_dict[temp_str[0]] = float(temp_str[2])
	index = 0
	for datum in train_data:
		c,u,d = datum.split('\t')
		rank_list.append(user_dict[u])
	for datum in cross_data:
		c,u,d = datum.split('\t')
		index += 1
		#print(str(index))
		rank_list.append(user_dict[u])
	for datum in test_data:
		c,u,d = datum.split('\t')
		index += 1
	#	print(str(index))
		rank_list.append(user_dict[u])

	return rank_list
def tokenize_me(sentence):
	 temp_list = [[w for w in word_tokenize(y)] for y in sent_tokenize(sentence)]
	 a = []
	 [a.extend(l) for l in temp_list]
	 b  = [l for l in a if(len(l)) > 2]
	 return b
class StemmedVectorizer(CountVectorizer):
	def build_analyzer(self):
		analyzer = super(StemmedVectorizer,self).build_analyzer()
		return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))
		
def get_vectorizers_as_feature(Xtrain):
	hash_tag = []
	y_rank = []
	len_tweets = []
	for elem in range(0,len(Xtrain.X)):
        	y_rank.append(Xtrain.y_rank[elem])
        	hash_tag.append([Xtrain.hash_tag[elem]])
		#Xtrain.latest_tweets[elem] = Xtrain.latest_tweets[elem].encode('utf-8','ignore')
       		Xtrain.latest_tweets[elem] = Xtrain.latest_tweets[elem].decode('utf-8','ignore')
		len_tweets.append([Xtrain.len_twits[elem]])
    #  print(Xtrain[elem][1][1])
	#vectorizer = CountVectorizer(min_df = 2,stop_words='english',ngram_range = (1,2),tokenizer = tokenize_me)
	vectorizer = StemmedVectorizer(min_df = 3 , stop_words='english',ngram_range = (1,2))#,tokenizer = tokenize_me)
	vectorizer_for_latest_tweets = StemmedVectorizer(min_df = 2, stop_words = 'english',ngram_range = (1,2))#,tokenizer = tokenize_me)
	#vectorizer_for_latest_tweets = CountVectorizer(min_df = 2,stop_words='english',ngram_range = (1,2),tokenizer = tokenize_me)
    	mat_latest = vectorizer_for_latest_tweets.fit_transform(Xtrain.latest_tweets)
    	matrix = vectorizer.fit_transform(Xtrain.X)
	#matrix = hstack([matrix,np.array(len_tweets)])
	storeModels(vectorizer_for_latest_tweets,2.2,'v')
	storeModels(vectorizer,2.1,'v')
	return matrix, mat_latest
def get_vectorizers_for_test(path_name = 'data/model/model_',Xtrain_ = None):
	hash_tag = []
	y_rank = []
	len_tweets = []
	for elem in range(0,len(Xtrain_.X)):
        	y_rank.append(Xtrain_.y_rank[elem])
        	hash_tag.append([Xtrain_.hash_tag[elem]])
		#Xtrain.latest_tweets[elem] = Xtrain.latest_tweets[elem].encode('utf-8','ignore')
       		Xtrain_.latest_tweets[elem] = Xtrain_.latest_tweets[elem].decode('utf-8','ignore')
		len_tweets.append([Xtrain_.len_twits[elem]])
	
	clf_1 = loadModels(path_name +'v'+'2.1')
	clf_2 = loadModels(path_name +'v'+'2.2')
	if(clf_1 == None):
		print('problem in loading models i am exiting')
		sys.exit(0)
	if(clf_2 == None):
		print('problem in loading model 2 i am exiting')
		sys.exit(0)
	mat_p = clf_1.transform(Xtrain_.X)
	#mat_p = hstack([mat_p,np.array(len_tweets)])
	mat_l = clf_2.transform(Xtrain_.latest_tweets)
	return mat_p,mat_l

def tweet_lenght(data_source,avg_len):
	mat_return = zeros((len(data_source),1))
	for idx,each in enumerate(data_source):
		temp_tweets = each.split(' ')
		if(len(temp_tweets) > avg_len):
			mat_return[idx][0] = 1
		else:
		 	mat_return[idx][0] = 0
	return mat_return
def latest_tweets_data_formation(data_source,latest_tweets_dict,tweet_contents_dict):
	X = []
	y = []
	latest_tweets_list = []
	user_names= []
	for datum in xrange(0,len(data_source)):
        	tokens = data_source[datum].split('\t')
		X.append(tokens[2])
		y.append(tokens[0])
		user_names.append(tokens[1])
        	if(latest_tweets_dict.get(tokens[1],None) != None):
            		latest_tweets_list.append(latest_tweets_dict[tokens[1]].decode('utf-8','ignore'))
        	else:                        
            		latest_tweets_list.append(tweet_contents_dict[tokens[1]].decode('utf-8','ignore'))  
	return latest_tweets_list,X,y,user_names
def shift_data(data_list_1,data_list_2,proportion):
	size_to = len(data_list_2)/proportion
	data_list_1.extend(data_list_2[:size_to])
	return data_list_1,data_list_2
def start_operating_on_data_new_format():
	global_feature_dict = {'pos_tag_pro':0,'pos_tag_latest':0,'con_pos_tag_pro':0,'con_pos_tag_latest':0,'tp_profile':0,'tp_latest':0,'urls':0,'rt_pro':0,'rt_latest':0,'punc_pro':0,'punc_latest':0,'tw_len_pro':0,'tw_len_latest':0,'polarity_pro':0,'polarity_latest':0,'user_men':0,'hash_tag':0,'em_profile':0,'em_latest':0,'usernames_pro':0}
	global_mat = {}
	k_fold_operation,valid_d = sys.argv[len(sys.argv) - 6:(len(sys.argv) - 4)]
	test_operation,valid_test = sys.argv[len(sys.argv) - 4:(len(sys.argv) - 2 ) ]
	train_test_operation,valid_train_test = sys.argv[len(sys.argv) - 2: ]
	print(k_fold_operation +' '+ str(valid_d))
	print(test_operation +' '+ valid_test)
	print(train_test_operation + ' ' + valid_train_test)
	sys.argv = sys.argv[:(len(sys.argv) - 6)]
	#k_fold_operation = True # true or false operation
	#file_to_tweet_contents = open('data/userTweetContent2.txt') # dont need it now as i combined the latest tweets and tweetContents
	#file_to_rank_list = open('data/userRTSRank.txt')
	train_data_source = read_data_from_source('data/training_set.txt')
	cross_data_source = read_data_from_source('data/cross_validation_set.txt')
	test_data_source = read_data_from_source('data/testing_set.txt')
	latest_tweets_train = read_data_from_source('data/latest_tweets_training_set.txt')
	latest_tweets_cross = read_data_from_source('data/latest_tweets_cross_validation_set.txt')
	latest_tweets_test = read_data_from_source('data/latest_tweets_testing_set.txt')
	tweet_contents = read_data_from_source('data/userTweetContent2.txt')
	rank_data = read_data_from_source('data/userRTSRank.txt')
	tokenizer_2 = RegexpTokenizer(' ',re.UNICODE)
	latest_tweets_dict = get_latest_tweets(latest_tweets_train,10)
	tweet_contents_dict = get_tweet_contents(tweet_contents,10)   
	latest_tweets_list1,X_train,Y_train,user_names_train = latest_tweets_data_formation(train_data_source,latest_tweets_dict,tweet_contents_dict)
	latest_tweets_list2,X_cross,Y_cross,user_names_cross = latest_tweets_data_formation(cross_data_source,latest_tweets_dict,tweet_contents_dict)
	latest_tweets_list3,X_test,Y_test, user_names_test = latest_tweets_data_formation(test_data_source,latest_tweets_dict,tweet_contents_dict)
	user_rank = get_rank_list(rank_data,train_data_source,cross_data_source,test_data_source)
	zippedXtrain = randomized_data(X_train,Y_train,user_rank,latest_tweets_train,user_names_train)
	zippedXtest = randomized_data(X_test,Y_test,user_rank,latest_tweets_test,user_names_cross)
	zippedXcross = randomized_data(X_cross,Y_cross,user_rank,latest_tweets_cross,user_names_test)
    	# for each in zippedXtest:
    	#   print(each)
	#kf = cross_validation.StratifiedKFold(X_update.y,4,indices = True) 
    	dictOfFeatures = startParsingField(zippedXtrain.X)
 	X_update_train = get_dic_to_list(dictOfFeatures,zippedXtrain)
	#kf = cross_validation.StratifiedKFold(X_update_train.y,5,indices = True) 
	matrix_p_train,matrix_l_train = get_vectorizers_as_feature(X_update_train)
	global_mat['pro_train'] = matrix_p_train
	global_mat['latest_train'] = matrix_l_train
	dictOfFeatures = startParsingField(zippedXcross.X)
	X_update_cross = get_dic_to_list(dictOfFeatures,zippedXcross)
	matrix_p_cross,matrix_l_cross = get_vectorizers_for_test(path_name = 'data/model/model_',Xtrain_ = X_update_cross)
	dictOfFeatures = startParsingField(zippedXtest.X)
	X_update_test = get_dic_to_list(dictOfFeatures,zippedXtest)
	matrix_p_test ,matrix_l_test = get_vectorizers_for_test(path_name = 'data/model/model_',Xtrain_ = X_update_test)
	#////////////////////for stratified update //////////////////////////////////
	print('here is the test for matrix size')
	print(str(matrix_p_train.shape) +'second '+ str(matrix_p_cross.shape))
	matrix_p_stratified_train = vstack([matrix_p_train, matrix_p_cross])
	matrix_p_stratified_kfold = vstack([matrix_p_stratified_train, matrix_p_test])
	print(str(matrix_p_stratified_train.shape))
	global_mat['stra_pro_train'] = matrix_p_stratified_train
	global_mat['stra_pro_kfold'] = matrix_p_stratified_kfold
	matrix_l_stratified_train = vstack([matrix_l_train, matrix_l_cross])
	matrix_l_stratified_kfold = vstack([matrix_l_stratified_train,matrix_l_test])
	print(str(matrix_l_stratified_train.shape)+'\n')
	global_mat['stra_latest_train'] = matrix_l_stratified_train
	global_mat['stra_latest_kfold'] = matrix_l_stratified_kfold
 	arr_1 = np.array(X_update_train.y)	
	arr_2 = np.array(X_update_cross.y)
	arr = np.concatenate([arr_1,arr_2])
	arr_test = np.array(X_update_test.y)
	arr_total = np.concatenate([arr,arr_test])
	user_name_list = X_update_train.user_names
	user_name_list.extend(X_update_cross.user_names)
	print(str(len(user_name_list)))
	#return
	user_name_list.extend(X_update_test.user_names)
	#return # for test only
	#class_labels = vstack([ X_update_train.y, X_update_cross.y]) 
	#kf = cross_validation.KFold(len(arr),5) 
	kf = cross_validation.StratifiedKFold(arr,n_folds = 5) 
	#///////////////user_name_features//////////////////////////////////////////
	arr_train = get_username_feature(train_data_source)
	arr_cross = get_username_feature(cross_data_source)
	arr_test = get_username_feature(test_data_source)
	arr_con = np.concatenate([arr_train,arr_cross])
	arr_con_test = np.concatenate([arr_con,arr_test])
	global_mat['stra_usernames_pro_kfold'] = arr_con_test
	global_mat['stra_usernames_pro_train'] = arr_con
	global_mat['usernames_pro_test'] = arr_test
	#/////////////////// get topic parsed feature mat ///////////////////////////
	topic_features_train = get_parse_topic_features('data/tutorial_composition_training0_3.txt',10)
	topic_features_cross = get_parse_topic_features('data/tutorial_composition_cross0_1.txt',10)
	topic_features_test = get_parse_topic_features('data/tutorial_composition_testing0_1.txt',10)	
	topic_latest_tweet_features_train = get_parse_topic_features('data/tutorial_composition_latest-tweets_training0_1.txt',20)
	topic_latest_tweet_features_cross = get_parse_topic_features('data/tutorial_composition_latest-tweets_cross0_1.txt',20)
	topic_latest_tweet_features_test = get_parse_topic_features('data/tutorial_composition_latest-tweets_testing0_1.txt',20)
	global_mat['tp_profile_train'] = topic_features_train
	global_mat['tp_profile_cross'] = topic_features_cross
	global_mat['tp_profile_test'] = topic_features_test
	global_mat['tp_latest_train'] = topic_latest_tweet_features_train
	global_mat['tp_latest_cross'] = topic_latest_tweet_features_cross
	global_mat['tp_latest_test'] = topic_latest_tweet_features_test
	#/////////////////////// for stratified train ////////////////////////////////////////
	tp_stratified_mat_p = vstack([topic_features_train, topic_features_cross])
	tp_stratified_mat_p_kfold = vstack([tp_stratified_mat_p,topic_features_test])
	global_mat['stra_tp_profile_train'] = tp_stratified_mat_p
	global_mat['stra_tp_profile_kfold'] = tp_stratified_mat_p_kfold
	tp_stratified_mat_l = vstack([topic_latest_tweet_features_train, topic_latest_tweet_features_cross])
	tp_stratified_mat_l_kfold = vstack([tp_stratified_mat_l,topic_latest_tweet_features_test])
	global_mat['stra_tp_latest_train'] = tp_stratified_mat_l
	global_mat['stra_tp_latest_kfold'] = tp_stratified_mat_l_kfold
	#/////////////////////////////////////////////////////////////////////////////////////
	#///////////////////////////////////////////////////////////////////////////////////
	#/////////////////////////////// get emoticon features mat /////////////////////////////////////////
	emoticon_features_train = get_emoticon_count(path_name = 'data/training_set.txt')
	emoticon_features_cross = get_emoticon_count(path_name = 'data/cross_validation_set.txt')
	emoticon_features_test = get_emoticon_count(path_name = 'data/testing_set.txt')
	emoticon_features_latest_train = get_emoticon_count(path_name = 'data/ecig_latest_tweets_pos_train.txt')
	emoticon_features_latest_cross = get_emoticon_count(path_name = 'data/ecig_latest_tweets_pos_cross.txt')
	emoticon_features_latest_test = get_emoticon_count(path_name = 'data/ecig_latest_tweets_pos_testing.txt')
	global_mat['em_profile_train'] = emoticon_features_train
	global_mat['em_profile_cross'] = emoticon_features_cross
	global_mat['em_profile_test'] = emoticon_features_test
	global_mat['em_latest_train'] = emoticon_features_latest_train
	global_mat['em_latest_cross'] = emoticon_features_latest_cross
	global_mat['em_latest_test'] = emoticon_features_latest_test

	#///////////////////////// emoticon features ////////////////////////////////////
	emoticon_stratified_mat_p = vstack([emoticon_features_train, emoticon_features_cross])
	emoticon_stratified_mat_p_kfold = vstack([emoticon_stratified_mat_p,emoticon_features_test])
	global_mat['stra_em_profile_train'] = emoticon_stratified_mat_p
	global_mat['stra_em_profile_kfold'] = emoticon_stratified_mat_p_kfold
	emoticon_stratified_mat_l = vstack([emoticon_features_latest_train, emoticon_features_latest_cross])
	emoticon_stratified_mat_l_kfold = vstack([emoticon_stratified_mat_l,emoticon_features_latest_test])
	global_mat['stra_em_latest_train'] = emoticon_stratified_mat_l
	global_mat['stra_em_latest_kfold'] = emoticon_stratified_mat_l_kfold
	#///////////////////////////////////////////////////////////////////////////////
	#/////////// //////// get pos tag features mat ////////////////////////////////////
	pos_tag_features_train = get_pos_tags('data/ecig_dataset_pos_train.txt')
	pos_tag_features_cross = get_pos_tags('data/ecig_dataset_pos_cross.txt')
	pos_tag_features_test = get_pos_tags('data/ecig_dataset_pos_test.txt')
	con_pos_tag_features_train_l = get_consecutive_pos_tag('data/ecig_dataset_pos_train.txt')
	print(str(len(con_pos_tag_features_train_l)))
	con_pos_tag_features_train = generates_features(con_pos_tag_features_train_l,3.3)
	con_pos_tag_features_cross_l = get_consecutive_pos_tag('data/ecig_dataset_pos_cross.txt')
	con_pos_tag_features_cross = generates_features_transform(con_pos_tag_features_cross_l,3.3,'v')
	con_pos_tag_features_test_l= get_consecutive_pos_tag('data/ecig_dataset_pos_test.txt')
	con_pos_tag_features_test = generates_features_transform(con_pos_tag_features_test_l,3.3,'v')
	pos_tag_features_latest_train = get_pos_tags('data/ecig_latest_tweets_pos_train.txt')
	pos_tag_features_latest_cross = get_pos_tags('data/ecig_latest_tweets_pos_cross.txt')
	pos_tag_features_latest_test = get_pos_tags('data/ecig_latest_tweets_pos_testing.txt')
	con_pos_tag_features_latest_train_l = get_consecutive_pos_tag('data/ecig_latest_tweets_pos_train.txt')
	con_pos_tag_features_latest_train = generates_features(con_pos_tag_features_latest_train_l,3.4)
	con_pos_tag_features_latest_cross_l = get_consecutive_pos_tag('data/ecig_latest_tweets_pos_cross.txt')
	con_pos_tag_features_latest_cross = generates_features_transform(con_pos_tag_features_latest_cross_l,3.4,'v')
	con_pos_tag_features_latest_test_l = get_consecutive_pos_tag('data/ecig_latest_tweets_pos_testing.txt')
	con_pos_tag_features_latest_test = generates_features_transform(con_pos_tag_features_latest_test_l,3.4,'v')
	#con_pos_tag_features = get_consecutive_pos_tag()
	print(str(pos_tag_features_train.shape))
	print(str(pos_tag_features_cross.shape))
	print(str(pos_tag_features_test.shape))
	print(str(con_pos_tag_features_latest_train.shape))
	print(str(con_pos_tag_features_latest_cross.shape))
	print(str(con_pos_tag_features_latest_test.shape))
	global_mat['pos_tag_pro_train'] = pos_tag_features_train
	global_mat['pos_tag_pro_cross'] = pos_tag_features_cross
	global_mat['pos_tag_pro_test'] = pos_tag_features_test
	global_mat['con_pos_tag_pro_train'] = con_pos_tag_features_train
	global_mat['con_pos_tag_pro_cross'] = con_pos_tag_features_cross
	global_mat['con_pos_tag_pro_test'] = con_pos_tag_features_test
	global_mat['pos_tag_latest_train'] = pos_tag_features_latest_train
	global_mat['pos_tag_latest_cross'] = pos_tag_features_latest_cross
	global_mat['pos_tag_latest_test'] = pos_tag_features_latest_test
	global_mat['con_pos_tag_latest_train'] = con_pos_tag_features_latest_train
	global_mat['con_pos_tag_latest_cross'] = con_pos_tag_features_latest_cross
	global_mat['con_pos_tag_latest_test'] = con_pos_tag_features_latest_test
	#/////////////// pos and con pos tag features //////////////////////////////////////////
	pos_stratified_train = vstack([pos_tag_features_train, pos_tag_features_cross])
	pos_stratified_kfold = vstack([pos_stratified_train,pos_tag_features_test])
	global_mat['stra_pos_tag_pro_train'] = pos_stratified_train
	global_mat['stra_pos_tag_pro_kfold'] = pos_stratified_kfold
	#////////////////////////////////////////////////////////////////////////////////////
	con_pos_stratified_train = vstack([con_pos_tag_features_train, con_pos_tag_features_cross])
	con_pos_stratified_kfold = vstack([con_pos_stratified_train,con_pos_tag_features_test])
	global_mat['stra_con_pos_tag_pro_train'] = con_pos_stratified_train
	global_mat['stra_con_pos_tag_pro_kfold'] = con_pos_stratified_kfold
	pos_stratified_latest_train = vstack([pos_tag_features_latest_train, pos_tag_features_latest_cross])
	pos_stratified_latest_kfold = vstack([pos_stratified_latest_train,pos_tag_features_latest_test])
	global_mat['stra_pos_tag_latest_train'] = pos_stratified_latest_train
	global_mat['stra_pos_tag_latest_kfold'] = pos_stratified_latest_kfold
	con_pos_stratified_latest_train = vstack([con_pos_tag_features_latest_train, con_pos_tag_features_latest_cross])
	con_pos_stratified_latest_kfold = vstack([con_pos_stratified_latest_train,con_pos_tag_features_latest_test])
	global_mat['stra_con_pos_tag_latest_train'] = con_pos_stratified_latest_train
	global_mat['stra_con_pos_tag_latest_kfold'] = con_pos_stratified_latest_kfold
	#/////////////////////////////////////////////////////////////////////////////////////
	#//////////////////////////////// get polarity features //////////////////////////
	sent_score,sent_score_without = load_sent_word_net(format_specifier = '_')
	polarity_feature_mat_train = get_polarity_features(train_data_source,sent_score,sent_score_without)
	polarity_feature_mat_cross = get_polarity_features(cross_data_source,sent_score,sent_score_without)
	polarity_feature_mat_test = get_polarity_features(test_data_source,sent_score,sent_score_without)
	polarity_feature_mat_latest_train = get_polarity_features(latest_tweets_train,sent_score,sent_score_without)
	polarity_feature_mat_latest_cross = get_polarity_features(latest_tweets_cross,sent_score,sent_score_without)
	polarity_feature_mat_latest_test = get_polarity_features(latest_tweets_test,sent_score,sent_score_without)
	global_mat['polarity_pro_train'] = polarity_feature_mat_train
	global_mat['polarity_pro_cross'] = polarity_feature_mat_cross
	global_mat['polarity_pro_test'] = polarity_feature_mat_test
	global_mat['polarity_latest_train'] = polarity_feature_mat_latest_train
	global_mat['polarity_latest_cross'] = polarity_feature_mat_latest_cross
	global_mat['polarity_latest_test'] = polarity_feature_mat_latest_test
	#/////////////////////////////////////////////////////////////////////////////////
	#///////////////////////////////// get rt feature /////////////////////////////////
	polarity_stratified_train = vstack([polarity_feature_mat_train, polarity_feature_mat_cross])
	polarity_stratified_kfold = vstack([polarity_stratified_train,polarity_feature_mat_test])
	global_mat['stra_polarity_pro_train'] = polarity_stratified_train
	global_mat['stra_polarity_pro_kfold'] = polarity_stratified_kfold
	polarity_stratified_train_latest = vstack([polarity_feature_mat_latest_train, polarity_feature_mat_latest_cross])
	polarity_stratified_kfold_latest = vstack([polarity_stratified_train_latest,polarity_feature_mat_latest_test])
	global_mat['stra_polarity_latest_train'] = polarity_stratified_train_latest
	global_mat['stra_polarity_latest_kfold'] = polarity_stratified_kfold_latest
	#/////////////////////////////////////////////////////////////////////////////////
	#///////////////////////////////// get rt feature /////////////////////////////////
	rt_feature_mat_train = get_rt_features(train_data_source)
	global_mat['rt_pro_train'] = rt_feature_mat_train
	rt_feature_mat_cross = get_rt_features(cross_data_source)
	global_mat['rt_pro_cross'] = rt_feature_mat_cross
	rt_feature_mat_test = get_rt_features(test_data_source)
	global_mat['rt_pro_test'] = rt_feature_mat_test
	rt_feature_mat_latest_train = get_rt_features(latest_tweets_train)
	global_mat['rt_latest_train'] = rt_feature_mat_latest_train 
	rt_feature_mat_latest_cross = get_rt_features(latest_tweets_cross)
	global_mat['rt_latest_cross'] = rt_feature_mat_latest_cross
	rt_latest_feature_mat_test = get_rt_features(latest_tweets_test)
	global_mat['rt_latest_test'] = rt_latest_feature_mat_test
	#////////////////rt stratified features////////////////////////////////////////////
	rt_stra_train_p = vstack([rt_feature_mat_train, rt_feature_mat_cross])
	rt_stra_kfold_p = vstack([rt_stra_train_p,rt_feature_mat_test])
	rt_stra_train_l = vstack([rt_feature_mat_latest_train, rt_feature_mat_latest_cross])
	rt_stra_kfold_l = vstack([rt_stra_train_l,rt_latest_feature_mat_test])
	global_mat['stra_rt_pro_train'] = rt_stra_train_p
	global_mat['stra_rt_latest_train'] = rt_stra_train_l
	global_mat['stra_rt_pro_kfold'] = rt_stra_kfold_p
	global_mat['stra_rt_latest_kfold'] = rt_stra_kfold_l
	#/////////////////////////////////////////////////////////////////////////////////
	#/////////////////////////////////////////////////////////////////////////////////
	#////////////////////////////////get punctuation feature //////////////////////////
	punc_feature_train = get_punctuation_feature(train_data_source)
	global_mat['punc_pro_train'] = punc_feature_train
	punc_feature_cross = get_punctuation_feature(cross_data_source)
	global_mat['punc_pro_cross'] = punc_feature_cross
	punc_feature_test = get_punctuation_feature(test_data_source)
	global_mat['punc_pro_test'] = punc_feature_test
	punc_feature_latest_train = get_punctuation_feature(latest_tweets_train)
	global_mat['punc_latest_train'] = punc_feature_latest_train
	punc_feature_latest_cross = get_punctuation_feature(latest_tweets_cross)
	global_mat['punc_latest_cross'] = punc_feature_latest_cross
	punc_feature_latest_test = get_punctuation_feature(latest_tweets_test)
	global_mat['punc_latest_test'] = punc_feature_latest_test
	#/////////////////////////////////////////////////////////////////////////////////
	#////////////////////////// punctuation stratified features///////////////////////
	punc_stratified_features_train_p = vstack([punc_feature_train, punc_feature_cross])
	punc_stratified_features_kfold_p = vstack([punc_stratified_features_train_p,punc_feature_test])
	global_mat['stra_punc_pro_train'] = punc_stratified_features_train_p
	global_mat['stra_punc_pro_kfold'] = punc_stratified_features_kfold_p
	punc_stratified_features_train_l = vstack([punc_feature_latest_train, punc_feature_latest_cross])
	punc_stratified_features_kfold_l = vstack([punc_stratified_features_train_l,punc_feature_latest_test])
	global_mat['stra_punc_latest_train'] = punc_stratified_features_train_l
	global_mat['stra_punc_latest_kfold'] = punc_stratified_features_kfold_l
	#////////////////////////////////////////////////////////////////////////////////
	#////////////////////// get tweets ///////////////////////////////////////////////
	tw_len_train = tweet_lenght(train_data_source,8)
	global_mat['tw_len_pro_train'] = tw_len_train
	tw_len_cross = tweet_lenght(cross_data_source,8)
	global_mat['tw_len_pro_cross'] = tw_len_cross
	tw_len_test = tweet_lenght(test_data_source,8) 
	global_mat['tw_len_pro_test'] = tw_len_test
	tw_latest_len_train = tweet_lenght(latest_tweets_train,10)
	global_mat['tw_len_latest_train'] = tw_latest_len_train
	tw_latest_len_cross = tweet_lenght(latest_tweets_cross,10)
	global_mat['tw_len_latest_cross'] = tw_latest_len_cross
	tw_latest_len_test = tweet_lenght(latest_tweets_test,10)
	global_mat['tw_len_latest_test'] = tw_latest_len_test
	#/////////////////////////// stratified tweet lenght features//////////////////////
	tw_len_stratified_train_p = vstack([tw_len_train, tw_len_cross])
	tw_len_stratified_kfold_p = vstack([tw_len_stratified_train_p,tw_len_test])
	global_mat['stra_tw_len_pro_train'] = tw_len_stratified_train_p
	global_mat['stra_tw_len_pro_kfold'] = tw_len_stratified_kfold_p
	tw_len_stratified_train_l = vstack([tw_latest_len_train, tw_latest_len_cross])
	tw_len_stratified_kfold_l = vstack([tw_len_stratified_train_l,tw_latest_len_test])
	global_mat['stra_tw_len_latest_train'] = tw_len_stratified_train_l
	global_mat['stra_tw_len_latest_kfold'] = tw_len_stratified_kfold_l
	#//////////////////////////////////////////////////////////////////////////////////
	#////////////////////////////////////////////////////////////////////////////////
	prog_name = sys.argv[0]
	list_of_combinations,final_list_elem = get_efficient_combinations(sys.argv[1:],len(sys.argv[1:]))
	'''for each_elem in list_of_combinations:
		for sub_each_elem in each_elem:
			print(sub_each_elem)
		print('\n')'''
#	if(1 == int(valid_multiple_threshold)):
		#work_with_folds(global_mat,matrix_p_stratified_train,matrix_l_stratified_train,['polarity_pro','polarity_latest','tp_profile','username_pro'],arr,kf,0.85,22,user_name_list)

	if(1 == int(valid_d)):
		parse_resultant_output(global_mat,'data/avg_shuffle_output/test_purpose_result_set.txt',matrix_p_stratified_train,matrix_l_stratified_train,arr,kf)
		print('we have done with k-fold operation so return now \n')
		return
	if(1 == int(valid_test)):
		print('Performing Train and Test Operation\n')
		train_model(global_mat,matrix_p_stratified_train,matrix_l_stratified_train,arr,['polarity_pro','polarity_latest','tp_profile','usernames_pro'],0.85)
		test_model(global_mat,matrix_p_test,matrix_l_test,arr_test,['polarity_pro','polarity_latest','tp_profile','usernames_pro'],0.85)
		print('End of Train and Test Operation\n')
		return
	if(1 == int(valid_train_test)):
		kf_l = cross_validation.StratifiedKFold(arr_total,n_folds = 5) 
		#train_test_operations(global_mat,matrix_p_stratified_kfold,matrix_l_stratified_kfold,arr_total,['pos_tag_pro','pos_tag_latest','polarity_pro','polarity_latest','tp_profile'],0.85)
		# con_pos_tag_pro, con_pos_tag_latest,pos_tag_latest_,polarity_pro,polarity_latest,tp_profile
		feature_elem =	['con_pos_tag_pro','con_pos_tag_latest','pos_tag_pro','pos_tag_latest','polarity_pro','polarity_latest','tp_profile']
		temp_features = feature_elem[:]
		index =	21 
		#for element in feature_elem:
			#temp_features.remove(element)	
		#work_with_folds(global_mat,matrix_p_stratified_train,matrix_l_stratified_train,['polarity_pro','polarity_latest','tp_profile','usernames_pro'],arr,kf,0.85,22,user_name_list)

		#train_test_operations(global_mat,matrix_p_stratified_kfold,matrix_l_stratified_kfold,arr_total,temp_features,0.85)
		with_shuffle_operation(global_mat,matrix_p_stratified_kfold,matrix_l_stratified_kfold,arr_total,temp_features,1.0,kf_l,index,user_name_list)
		index += 1

		print('end of operation so returning\n')
		return
	pool = Pool(processes = 23 )
	manager = Manager()
	lock = manager.Lock()
	shuffle_index = 0
	output_file_index = 18
	if(validation_check(final_list_elem,global_feature_dict)):
		print('parameter options are wrong')
		sys.exit(0)
	while(True):
		if(shuffle_index  == 1):
			break
		shuffle_index += 1
		for alpha in  np.arange(0.7,0.95,0.01):
			#for each in list_of_combinations:
			#sparse_mat,tuples.y = shuffle(matrix_p,matrix_l,pos_tag_features,con_pos_tag_features,rt_feature_mat,rt_latest_feature_mat,punc_feature,tw_len,tw_latest_len,tuples.y,random_state = 0,)
			li_com = list_of_combinations[:]
			p_mat = csr_matrix(matrix_p_stratified_train,copy = True)
			l_mat = csr_matrix(matrix_l_stratified_train,copy = True)
			#p_mat_c = csr_matrix(matrix_p_cross,copy = True)
			#l_mat_c = csr_matrix(matrix_l_cross,copy = True)
			g_mat = global_mat.copy()
			b_val= alpha
			s_index = shuffle_index
			#tup_y = X_update_train.y[:]
			#tup_c = X_update_cross.y[:]
			arr_y = arr[:]
			#print(li_com)
			print('starting the pool of threads')
			#pool.apply_async(work_with_combinations,(li_com,tup_y,tup_c,p_mat,l_mat,p_mat_c,l_mat_c,g_mat,b_val,s_index,lock,buildingClassifier,kf,output_file_index),callback = grab_result )    # just remain unused for testing
			'''pool.apply_async('''
			parse_resultant_output(li_com,global_mat,'data/avg_shuffle_output/xvalidation_purpose_result_set.txt',matrix_p_stratified_train,matrix_l_stratified_train,arr_y,kf,b_val)
	pool.close()
	pool.join()
	'''list_of_result = build_classifier(matrix_p,matrix_l,X_update.y,2.1,kf)
				fscore_result_F1 = []
				fscore_result_P = []
				fscore_result_R = []
		#print(str(list_of_result))
    				for sub_sub_each in list_of_result:
					F1,P,R = scoreMeasurement(sub_sub_each,beta_val)
    					fscore_result_F1.append(F1)
					fscore_result_P.append(P)
					fscore_result_R.append(R)
				p,r,f1 = 0,0,0
				if(result_set.F1 < float(sum(fscore_result_F1))/len(fscore_result_F1)):
					beta = beta_val
					best_combination = each[:]
					result_set.F1 = float(sum(fscore_result_F1))/len(fscore_result_F1)
					result_set.P = float(sum(fscore_result_P))/len(fscore_result_P)
        				result_set.R =  float(sum(fscore_result_R))/len(fscore_result_R)
	'''
	return
def start_opearting_on_data():
    file_object_ToRead_LowRank = open('data/topLowRanked120Profile.txt')
    file_object_ToRead_HighRank = open('data/topRanked60Profile.txt')
    file_object_Toread_testData = open('data/randomTestProfileData.txt')#non_empty_unique_profile.txt')
    file_object_rankList = open('data/userRTSRank.txt')
    file_to_latest_tweets = open('data/latest_tweet_output.txt')
    file_to_tweet_contents = open('data/userTweetContent2.txt')
    tokenizer = RegexpTokenizer('\t',re.UNICODE)
    tokenizer2 = RegexpTokenizer(' ',re.UNICODE)
    X = []
    y = []
    y_rank = []
    X_test = []
    y_test = []
    y_rank_test = []
    zippedX = []
    zippedXtest = []
    userDict = {}
    latest_tweets = {}
    latest_tweets_list = []
    latest_tweets_list_test = []
    tweet_contents = {}
    try:
        data = file_to_latest_tweets.readlines()
    finally:
        file_to_latest_tweets.close()
    for datum in data:
        tempStr = datum.split('\t')
        for i in xrange(1,max((len(tempStr) - 2),2)):#prev max used was 7 or you may try with 5
            if(latest_tweets.get(tempStr[0],None) != None):
                latest_tweets[tempStr[0]] = latest_tweets[tempStr[0]] + tempStr[i] +' '
            else:
                if(len(tempStr) > 1):
                    latest_tweets[tempStr[0]] = tempStr[i] + ' ' 
    try:
        data = file_to_tweet_contents.readlines()
    finally:
        file_to_tweet_contents.close()
    tweet_counter = {}
    for datum in data:
        tempStr = datum.split('\t')
        if(tweet_counter.get(tempStr[1],None) == None):
            tweet_counter[tempStr[1]] = 0
        tweet_counter[tempStr[1]] += 1
        if(tweet_counter[tempStr[1]] < 8):# previously it was 3    
            if(tweet_contents.get(tempStr[1],None) != None):
                tweet_contents[tempStr[1]] = tweet_contents[tempStr[1]] + tempStr[3]+' '
            else:
                tweet_contents[tempStr[1]] = tempStr[3] + ' '
    try:
        data = file_object_rankList.readlines()
    finally:
        file_object_rankList.close()
    for datum in data:
        tokens =  tokenizer2.tokenize(datum)
        userDict[tokens[0]] = float(tokens[2]) 
    try:
        data = file_object_ToRead_HighRank.readlines()
    finally:
        file_object_ToRead_HighRank.close()
    line = 0
    for datum in xrange(0,len(data)-200):
        tokens = tokenizer.tokenize(data[datum])
        if(latest_tweets.get(tokens[0],None) != None):
            latest_tweets_list.append(latest_tweets[tokens[0]].decode('utf-8','ignore'))
        else:                        
            latest_tweets_list.append(tweet_contents[tokens[0]].decode('utf-8','ignore'))  
        '''if(latest_tweets.get(tokens[0],None) != None):
            pass 
        else:
            tokens[1] = tokens[1] + ' ' + tweet_contents[tokens[0]]                                                                                                                                                '''
	X.append(tokens[1].decode('utf-8','ignore'))
        y_rank.append([userDict.get(tokens[0],0)])
        y.append(1) # it was one class
    
    try:
        data = file_object_ToRead_LowRank.readlines()
    finally:
        file_object_ToRead_LowRank.close()
    for datum in xrange(0,len(data)):
        tokens  = tokenizer.tokenize(data[datum])
	'''
        if(latest_tweets.get(tokens[0],None) != None):
            tokens[1] = tokens[1].decode('utf-8','ignore') + ' ' + latest_tweets[tokens[0]].decode('utf-8','ignore')
        else:
            tokens[1] = tokens[1].decode('utf-8','ignore') + ' ' + tweet_contents[tokens[0]].decode('utf-8','ignore')
	print(tokens[1])'''
	#tokens[1] = tokens[1].encode('utf-8','ignore')
        X.append(tokens[1].decode('utf-8','ignore'))
        y_rank.append([userDict.get(tokens[0],0)])
        if(latest_tweets.get(tokens[0],None) != None):
            latest_tweets_list.append(latest_tweets[tokens[0]].decode('utf-8','ignore'))
        else:
            latest_tweets_list.append(tweet_contents[tokens[0]].decode('utf-8','ignore'))
        y.append(0) # it was the zero class
    zippedX = randomized_data(X,y,y_rank,latest_tweets_list)
    #zippedX = zip(X,zip(y,y_rank))
    #print(zippedX)
    #for each in zippedX:
    #    print(each)
    try:
        data = file_object_Toread_testData.readlines()
    finally:
        file_object_Toread_testData.close()
    line = 0
    user_names_list_test = []
    bokens = ''
    for datum in xrange(0,len(data)):
        tokens  = tokenizer.tokenize(data[datum])
        if(latest_tweets.get(tokens[0],None) != None ):#and tokens[2].strip() == 1):
            bokens = latest_tweets[tokens[0]].decode('utf-8','ignore')
        else:#if(tokens[2].strip() == 1):
            bokens = tweet_contents.get(tokens[0],'').decode('utf-8','ignore')
	#tokens[1] = tokens[1].encode('utf-8','ignore')
	latest_tweets_list_test.append(bokens)
        X_test.append(tokens[1].decode('utf-8','ignore'))
        y_rank_test.append([userDict.get(tokens[0],0)])
        '''if(latest_tweets.get(tokens[0],None) != None):
            latest_tweets_list_test.append(latest_tweets[tokens[0]])
        else:
            latest_tweets_list_test.append(tweet_contents.get(tokens[0],''))'''
        #if(int(tokens[2].strip()) != 0 and int(tokens[2].strip()) != 1):
        #  print('lines: ' + str(tokens[2]))
        y_test.append(int(tokens[2].strip())) #temporarily we dont need that
        user_names_list_test.append(tokens[0])
    #  print(X_test[line]+" "+ str(y_test[line]))
        line += 1
    serial = 0
    for each in X_test:
        print(str(serial) + ' serial number data : ' + each[:20]+ ' class label: '+ str(y_test[serial]))
        serial += 1
    zippedXtest = randomized_data(X_test,y_test,y_rank_test,latest_tweets_list_test)
    # for each in zippedXtest:
    #   print(each)
    dictOfFeatures = startParsingField(zippedX.X)
    print(len(dictOfFeatures))
    #print(zippedXtest)
    X_update = get_dic_to_list(dictOfFeatures,zippedX)
    serial = 0
    
    #print(len(X_update))
    #print(X_update)
    #for each in xrange(0,len(X_update)):
    #   print(X_update[each],y[each],y_rank[each])
    del dictOfFeatures
    dictOfFeatures = startParsingField(zippedXtest.X)
    X_update_test = get_dic_to_list(dictOfFeatures,zippedXtest)
    for each in X_update_test.X:
        print(str(serial) + ' serial number data : ' + each[:20]+ ' class label: '+ str(X_update_test.y[serial]))
        serial += 1
    #print(X_update_test)
    # for elem in xrange(0,len(X_update[0])):
    #   print('Elemenet :' +str( X_update[elem][0] + ' '+ str(X_update[elem][1]))) 
    vectorizer = startTrainingOperation(X_update,4.1)
    get_user_list = startTesting(X_update_test, 4.1)
    '''file_to_write_user_names = open('C:\\TwitterBigData\\profileOutput\\users_of_promoters.txt','w+')
    for i in xrange(0,len(get_user_list)):
        if(get_user_list[i] == 1 or get_user_list[i]==True):
            file_to_write_user_names.write(user_names_list_test[i]+'\n')
    file_to_write_user_names.close()'''
            
    #svmClassifier(X_update,X_update_test)
    return
def buildingClassifier(modelIndex):
    global matrix
    global class_label
    write_score = open("C:\\TwitterBigData\\Scores\\report.txt","w+")
    kf = cross_validation.KFold(len(class_label),4,indices = True)
    for train_index,test_index in kf:
        X_train,X_test = matrix[train_index],matrix[test_index]
        Y_train,Y_test = class_label[train_index],class_label[test_index]
        logRegression = linear_model.LogisticRegression()
        print('start operation')
        score =logRegression.fit(X_train, Y_train)
        predictions = logRegression.predict(X_test)
        expected = Y_test
        write_score.write(classification_report(expected, predictions))
    #print("Accuracy %0.2f (+/- %0.2f)"%(score.mean(),score.std()*2))
    storeModels(logRegression, modelIndex)
    write_score.close()
   # logRegression.predict(matri)
    print(score)
    return
def printme(string):
    print(string)
    return
def __main__():
    print('here i am working')
    a = input('Input your choice [1-2]: ')
    modelIndex = input('model number: ')
    startOperatingandLoad()
    start_opearting_on_data()
     # 1:{startReordering("C:\\TwitterBigData\\tweetMessage\\tweetContentQuitkeyWords3.txt")},
    '''  startOperatingandLoad()
    startParsingField()
    dataFormatandMatrix(), 
    buildingClassifier(modelIndex)#{combineClasses()},'''
     
    
   
    return

def read_non_empty_profiles():
    read_non_empty_profile = open('C:\\TwitterBigData\\profileOutput\\New folder\\uniqueoutputProcessedProfile.txt')
    write_to_non_empty_profile = open('C:\\TwitterBigData\\profileOutput\\New folder\\non_empty_unique_profile.txt','w+')
    read_tweet_contents = open('C:\\TwitterBigData\\tweetContent\\userTweetContent2.txt')
    non_empty_users = {}
    
    try:
        data = read_non_empty_profile.readlines()
    finally:
        read_non_empty_profile.close()
    for datum in data:
        tempStr = datum.split('\t')
        if(len(tempStr) == 3):
            non_empty_users[tempStr[1]] = 0
            write_to_non_empty_profile.write(tempStr[1]+'\t'+tempStr[2])
    write_to_non_empty_profile.close()
    try:
        data = read_tweet_contents.readlines()
    finally:
        read_tweet_contents.close()
    counter = 0
    for datum in data:
        tempStr = datum.rstrip().split('\t')
        if(non_empty_users.get(tempStr[1],None) != None):
            counter += 1
            non_empty_users[tempStr[1]] += 1
    print('number of times the non-empty profile users tweeted are: ' + str(counter))    
            
            
    return
def get_profiles_for_flavors_quit():
    read_from_flavor_to_profile = open('C:\\TwitterBigData\\tweetContent\\flavors_file.txt')
    read_from_quit_to_profile = open('C:\\TwitterBigData\\tweetContent\\quit_cess_stop_file.txt')
def __main__():
	start_operating_on_data_new_format()
	return
__main__()
